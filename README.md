# CanonicalVoting
Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes, with arxiv report [here](https://arxiv.org/abs/2011.12001).
![Visualization](figures/visualize.png)
## Introduction
Canonical Voting is a 3D detection method that disentangles the direct offset into Local Canonical Coordinates (LCC), box scales and box orientations. Only LCC and box scales are regressed while box orientations are generated by a canonical voting scheme. Finally, a LCC-aware back-projection checking algorithm iteratively cuts out bounding boxes from the generated vote maps, with the elimination of false positives. Our model achieves state-of-the-art performance on challenging large-scale datasets of real point cloud scans: ScanNet and SceneNN.

## Dependencies
- [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine) v0.4.2
- Install our custom Hough Voting module under `houghvoting` folder, by running `python setup.py install`
- Tested with PyTorch v1.3.1 + CUDA 10.0
- Other dependecies: 
```
pip install hydra-core scipy scikit-learn tqdm shapely numpy-quaternion pickle plyfile
```

## Train and Test on ScanNet
You will need to first download [ScanNet](https://github.com/ScanNet/ScanNet) original dataset and [Scan2CAD](https://github.com/skanti/Scan2CAD) labels with oriented bounding boxes. Then download Scan2CAD model segments [here](https://drive.google.com/drive/folders/1yKIcQuJte9vToRLbZYgwdYqUDECBYs1T?usp=sharing) and then download our preprocessed ground-truth boxes [here](https://drive.google.com/drive/folders/1i4ctu3oxwYG19kczqNgryj5uMnZVQZCv?usp=sharing) for evaluation. Adjust their path accordingly in `config/config.yaml`.

To train model jointly for all categories, with one unified model:
```
python train_joint.py
```
To train model separately for each category:
```
python train_separate.py category=03211117,04379243,02808440,02747177,04256520,03001627,02933112,02871439,others -m
```
To eval model jointly:
```
python eval_joint.py
```
To eval model separately:
```
python eval_separate.py
```

## Pretrained Models
Pretrained models for both joint and separate training settings can be found [here](https://drive.google.com/drive/folders/1Af5mRVwwI370txOREXkooea8nK_SwzGk?usp=sharing). You will get about 15.4 mAP and 20.9 mAP for joint and separate training settings, respectively.

## Citation
If you find our algorithm useful, please consider citing:
```
@article{you2020canonical,
  title={Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes},
  author={You, Yang and Ye, Zelin and Lou, Yujing and Li, Chengkun and Li, Yong-Lu and Ma, Lizhuang and Wang, Weiming and Lu, Cewu},
  journal={arXiv preprint arXiv:2011.12001},
  year={2020}
}
```

## TODOs
- [ ] SceneNN dataset evaluation
- [ ] Interactive demo
